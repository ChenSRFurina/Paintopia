// æ™ºèƒ½èŠå¤©åŠ©æ‰‹è§†å›¾
// é›†æˆFractFlowåç«¯ï¼Œæ”¯æŒLLMå¯¹è¯ã€VLMå›¾åƒåˆ†æã€è®°å¿†å­˜å‚¨å’Œå®šæ—¶æé—®

import SwiftUI
import AVFoundation

struct ChatbotView: View {
    @Binding var canvasImage: UIImage?
    @Binding var paths: [PathSegment]
    @Binding var isObservingCanvas: Bool
    
    @StateObject private var apiClient = ChatbotAPIClient.shared
    @StateObject private var audioRecorder = AudioRecorder()
    @EnvironmentObject var navigationManager: NavigationManager
    
    @State private var messages: [EnhancedChatMessage] = []
    @State private var inputText = ""
    @State private var isLoading = false
    @State private var isRecording = false
    @State private var isUploading = false
    @State private var recognitionError: String?
    @State private var audioPlayer: AVAudioPlayer?
    
    // è‡ªåŠ¨åˆ†æç›¸å…³
    @State private var autoAnalysisTimer: Timer?
    @State private var lastAnalysisTime: Date?
    private let analysisInterval: TimeInterval = 30.0 // 30ç§’é—´éš”
    
    @State private var connectionStatus: String = "æœªè¿æ¥"
    @State private var connectionColor: Color = .gray
    
    init(canvasImage: Binding<UIImage?> = .constant(nil), paths: Binding<[PathSegment]> = .constant([]), isObservingCanvas: Binding<Bool> = .constant(false)) {
        self._canvasImage = canvasImage
        self._paths = paths
        self._isObservingCanvas = isObservingCanvas
    }
    
    var body: some View {
        if isObservingCanvas {
            HStack {
                Spacer()
                Text("æ­£åœ¨è§‚å¯Ÿç”»å¸ƒï¼Œè¯·ç¨ç­‰...")
                    .font(.caption)
                    .foregroundColor(.orange)
                    .padding(.top, 8)
                Spacer()
            }
        }
        HStack(spacing: 0) {
            // å¤´åƒåŒºåŸŸ
            VStack {
                ZStack {
                    Image("avatar")
                        .resizable()
                        .aspectRatio(contentMode: .fit)
                        .frame(width: 48, height: 48)
                        .clipShape(Circle())
                        .overlay(
                            Circle()
                                .stroke(connectionColor, lineWidth: 2)
                        )
                    
                    // è¿æ¥çŠ¶æ€æŒ‡ç¤ºå™¨
                    if isLoading {
                        Circle()
                            .fill(Color.blue.opacity(0.3))
                            .frame(width: 52, height: 52)
                            .overlay(
                                ProgressView()
                                    .scaleEffect(0.6)
                                    .progressViewStyle(CircularProgressViewStyle(tint: .white))
                            )
                    }
                }
                .padding(.top, 20)
                
                // è¿æ¥çŠ¶æ€æ–‡æœ¬
                Text(connectionStatus)
                    .font(.caption2)
                    .foregroundColor(.secondary)
                    .padding(.top, 4)
                
                Spacer()
            }
            .frame(width: 60)
            
            // èŠå¤©å†…å®¹åŒºåŸŸ - æ‚¬æµ®æ°”æ³¡å½¢å¼
            VStack(spacing: 0) {
                // å¤´éƒ¨
                HStack {
                    VStack(alignment: .leading) {
                        Text("å°ç”»")
                            .font(.headline)
                            .fontWeight(.bold)
                            .foregroundColor(.blue)
                        
                        if let sessionId = apiClient.currentSessionId {
                            Text("ä¼šè¯: \(sessionId.prefix(8))...")
                                .font(.caption2)
                                .foregroundColor(.secondary)
                        }
                    }
                    
                    Spacer()
                    
                    // åŠŸèƒ½æŒ‰é’®
                    HStack(spacing: 8) {
                        // åˆ†æç”»å¸ƒæŒ‰é’®
                        Button(action: analyzeCurrentCanvas) {
                            Image(systemName: "eye.fill")
                                .font(.caption)
                                .foregroundColor(.white)
                        }
                        .frame(width: 24, height: 24)
                        .background(Color.blue)
                        .cornerRadius(6)
                        .disabled(isLoading || paths.isEmpty)
                        
                        // æ–°ä¼šè¯æŒ‰é’®
                        Button(action: startNewSession) {
                            Image(systemName: "plus.circle.fill")
                                .font(.caption)
                                .foregroundColor(.white)
                        }
                        .frame(width: 24, height: 24)
                        .background(Color.green)
                        .cornerRadius(6)
                        .disabled(isLoading)
                    }
                }
                .padding(.horizontal, 16)
                .padding(.top, 16)
                .padding(.bottom, 8)
                
                // æ¶ˆæ¯åˆ—è¡¨ - æ‚¬æµ®æ°”æ³¡
                ScrollViewReader { proxy in
                    ScrollView {
                        LazyVStack(spacing: 12) {
                            ForEach(messages) { message in
                                FloatingChatBubble(message: message)
                                    .id(message.id)
                                    .transition(.asymmetric(
                                        insertion: .scale.combined(with: .opacity),
                                        removal: .opacity
                                    ))
                                    .animation(.spring(response: 0.5, dampingFraction: 0.8), value: message.id)
                            }
                            
                            if isLoading {
                                HStack {
                                    ProgressView()
                                        .scaleEffect(0.8)
                                    Text("å°ç”»æ­£åœ¨æ€è€ƒ...")
                                        .font(.caption)
                                        .foregroundColor(.secondary)
                                    Spacer()
                                }
                                .padding(.horizontal, 16)
                                .id("loading")
                            }
                        }
                        .padding(.horizontal, 16)
                        .padding(.bottom, 20) // åº•éƒ¨ç•™ç™½
                    }
                    .frame(maxHeight: 400) // é™åˆ¶é«˜åº¦
                    .onChange(of: messages.count) { _ in
                        if let lastMessage = messages.last {
                            withAnimation(.easeInOut(duration: 0.3)) {
                                proxy.scrollTo(lastMessage.id, anchor: .bottom)
                            }
                        }
                    }
                }
                
                Spacer()
            }
        }
        .frame(width: 300)
        .background(
            RoundedRectangle(cornerRadius: 20)
                .fill(.ultraThinMaterial)
                .shadow(color: .black.opacity(0.1), radius: 10, x: 0, y: 5)
        )
        .padding(.leading, 16)
        .padding(.top, 16)
        .overlay(
            // æ‚¬æµ®éº¦å…‹é£æŒ‰é’®
            VStack {
                Spacer()
                HStack {
                    Spacer()
                    Button(action: {
                        if audioRecorder.isRecording {
                            stopRecordingAndRecognize()
                        } else if !isUploading {
                            startRecording()
                        }
                    }) {
                        ZStack {
                            Circle()
                                .fill(audioRecorder.isRecording ? Color.red : Color.blue)
                                .frame(width: 56, height: 56)
                                .shadow(color: .black.opacity(0.3), radius: 4, x: 0, y: 2)
                            
                            // å½•éŸ³æ—¶çš„è„‰å†²åŠ¨ç”»
                            if audioRecorder.isRecording {
                                Circle()
                                    .stroke(Color.red.opacity(0.3), lineWidth: 2)
                                    .frame(width: 72, height: 72)
                                    .scaleEffect(1.5)
                                    .opacity(0)
                                    .animation(
                                        Animation.easeInOut(duration: 1.5)
                                            .repeatForever(autoreverses: false),
                                        value: audioRecorder.isRecording
                                    )
                            }
                            
                            Image(systemName: audioRecorder.isRecording ? "mic.fill" : "mic")
                                .foregroundColor(.white)
                                .font(.system(size: 24, weight: .bold))
                        }
                        .scaleEffect(audioRecorder.isRecording ? 1.1 : 1.0)
                        .animation(.easeInOut(duration: 0.2), value: audioRecorder.isRecording)
                    }
                    .accessibilityLabel(audioRecorder.isRecording ? "æ­£åœ¨å½•éŸ³ï¼Œç‚¹å‡»ç»“æŸ" : "ç‚¹å‡»å¼€å§‹è¯­éŸ³è¾“å…¥")
                    .disabled(isUploading)
                    .padding(.trailing, 16)
                    .padding(.bottom, 16)
                    
                    // å½•éŸ³çŠ¶æ€æç¤º
                    if audioRecorder.isRecording {
                        Text("æ­£åœ¨å½•éŸ³...")
                            .font(.caption)
                            .foregroundColor(.red)
                            .padding(.trailing, 16)
                            .padding(.bottom, 8)
                    } else if isUploading {
                        Text("è¯†åˆ«ä¸­...")
                            .font(.caption)
                            .foregroundColor(.blue)
                            .padding(.trailing, 16)
                            .padding(.bottom, 8)
                    }
                }
            }
        )
        .onAppear {
            initializeChatbot()
        }
        .onDisappear {
            stopAutoAnalysis()
        }
        .onChange(of: paths) { newPaths in
            handleCanvasChange()
        }
    }
    
    // MARK: - è®¡ç®—å±æ€§
    
    private func initializeChatbot() {
        connectionStatus = "åˆå§‹åŒ–ä¸­..."
        
        // åˆ›å»ºæ–°ä¼šè¯
        apiClient.createNewSession { result in
            DispatchQueue.main.async {
                switch result {
                case .success(let response):
                    if response.success {
                        self.connectionStatus = "å·²è¿æ¥"
                        self.connectionColor = .green
                        // æ·»åŠ æ¬¢è¿æ¶ˆæ¯
                        let welcomeMessage = EnhancedChatMessage(
                            text: response.message ?? "å“ˆå–½å°æœ‹å‹ï¼Œæˆ‘æ˜¯å°ç”»ï¼æœ‰ä»€ä¹ˆæƒ³å’Œæˆ‘åˆ†äº«çš„å‘€ï¼Ÿæˆ‘å¯ä»¥çœ‹ä½ çš„ç”»å“¦ï½",
                            isUser: false,
                            messageType: .text
                        )
                        self.messages.append(welcomeMessage)
                        
                        // å¼€å§‹è‡ªåŠ¨åˆ†æå®šæ—¶å™¨
                        self.startAutoAnalysis()
                    } else {
                        self.connectionStatus = "è¿æ¥å¤±è´¥"
                        self.connectionColor = .gray
                        print("âŒ ä¼šè¯åˆ›å»ºå¤±è´¥: \(response.error ?? "æœªçŸ¥é”™è¯¯")")
                    }
                case .failure(let error):
                    self.connectionStatus = "è¿æ¥å¤±è´¥"
                    self.connectionColor = .gray
                    print("âŒ ç½‘ç»œè¿æ¥å¤±è´¥: \(error.localizedDescription)")
                }
            }
        }
    }
    
    private func startNewSession() {
        isLoading = true
        connectionStatus = "åˆ›å»ºæ–°ä¼šè¯..."
        
        apiClient.createNewSession { result in
            DispatchQueue.main.async {
                self.isLoading = false
                switch result {
                case .success(let response):
                    if response.success {
                        self.connectionStatus = "å·²è¿æ¥"
                        self.connectionColor = .green
                        // æ¸…ç©ºæ¶ˆæ¯å¹¶æ·»åŠ æ–°çš„æ¬¢è¿æ¶ˆæ¯
                        self.messages.removeAll()
                        let welcomeMessage = EnhancedChatMessage(
                            text: "æ–°çš„ç»˜ç”»å¯¹è¯å¼€å§‹å•¦ï¼æˆ‘æ˜¯å°ç”»ï¼Œå‡†å¤‡å¥½å’Œä½ ä¸€èµ·åˆ›ä½œäº†ï½",
                            isUser: false,
                            messageType: .text
                        )
                        self.messages.append(welcomeMessage)
                        self.startAutoAnalysis()
                    } else {
                        self.connectionStatus = "è¿æ¥å¤±è´¥"
                        self.connectionColor = .gray
                    }
                case .failure(_):
                    self.connectionStatus = "è¿æ¥å¤±è´¥"
                    self.connectionColor = .gray
                }
            }
        }
    }
    
    // MARK: - å›¾åƒåˆ†æåŠŸèƒ½
    
    private func analyzeCurrentCanvas() {
        print("ğŸ” å¼€å§‹åˆ†æç”»å¸ƒ...")
        print("ğŸ” å½“å‰è·¯å¾„æ•°é‡: \(paths.count)")
        
        guard !paths.isEmpty else {
            let hintMessage = EnhancedChatMessage(
                text: "ç”»å¸ƒä¸Šè¿˜æ²¡æœ‰å†…å®¹å‘¢ï¼Œå…ˆç”»ä¸€äº›ä¸œè¥¿è®©æˆ‘çœ‹çœ‹å§ï½",
                isUser: false,
                messageType: .text
            )
            messages.append(hintMessage)
            return
        }
        
        // åˆ›å»ºç”»å¸ƒæˆªå›¾
        let canvasImage = createCanvasScreenshot()
        guard let image = canvasImage else {
            let errorMessage = EnhancedChatMessage(
                text: "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨çœ‹ä¸æ¸…ä½ çš„ç”»ï¼Œè¯·ç¨åå†è¯•ï½",
                isUser: false,
                messageType: .error
            )
            messages.append(errorMessage)
            return
        }
        
        isLoading = true
        
        // æ·»åŠ åˆ†ææç¤ºæ¶ˆæ¯
        let analysisMessage = EnhancedChatMessage(
            text: "è®©æˆ‘çœ‹çœ‹ä½ ç”»çš„æ˜¯ä»€ä¹ˆ...",
            isUser: false,
            messageType: .analysis
        )
        messages.append(analysisMessage)
        
        // ä½¿ç”¨æ–°çš„observeAndReplyæ¥å£
        print("ğŸ” è°ƒç”¨observeAndReplyæ¥å£...")
        apiClient.observeAndReply(image) { result, rawJson in
            DispatchQueue.main.async {
                self.isLoading = false
                
                // æ£€æŸ¥rawJsonä¸­çš„audio_data
                if let audioBase64 = rawJson?["audio_data"] as? String, !audioBase64.isEmpty {
                    print("æ”¶åˆ°éŸ³é¢‘æ•°æ®ï¼Œé•¿åº¦: \(audioBase64.count) å­—ç¬¦")
                    // æ’­æ”¾ä»åç«¯è¿”å›çš„éŸ³é¢‘
                    self.playAudioFromBase64(audioBase64)
                    print("âœ… å·²æ’­æ”¾åç«¯è¿”å›çš„éŸ³é¢‘")
                } else {
                    print("æœªæ£€æµ‹åˆ°audio_dataæˆ–å†…å®¹ä¸ºç©º")
                }
                
                switch result {
                case .success(let response):
                    if response.success {
                        // ç§»é™¤åˆ†ææç¤ºæ¶ˆæ¯
                        if let lastMessage = self.messages.last, lastMessage.messageType == .analysis {
                            self.messages.removeLast()
                        }
                        
                        // ä¼˜å…ˆä½¿ç”¨vision_descä½œä¸ºå›å¤å†…å®¹ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨llm_reply
                        let replyText = !response.visionDesc.isEmpty ? response.visionDesc : response.llmReply
                        
                        // æ£€æŸ¥å›å¤å†…å®¹æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                        let errorKeywords = ["å°ç”»éœ€è¦å…ˆçœ‹çœ‹", "è¯·æˆªå›¾", "çœ‹ä¸æ¸…æ¥š"]
                        let containsError = errorKeywords.contains { keyword in
                            replyText.contains(keyword)
                        }
                        
                        // å¦‚æœåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œä½¿ç”¨vision_descæˆ–é»˜è®¤å›å¤
                        let finalReplyText: String
                        if containsError && !response.visionDesc.isEmpty {
                            finalReplyText = response.visionDesc
                            print("âš ï¸ æ£€æµ‹åˆ°é”™è¯¯å›å¤ï¼Œä½¿ç”¨vision_desc: \(response.visionDesc)")
                        } else if containsError {
                            finalReplyText = "æˆ‘çœ‹åˆ°ä½ ç”»äº†ä¸€äº›å¾ˆæœ‰è¶£çš„ä¸œè¥¿ï¼èƒ½å‘Šè¯‰æˆ‘ä½ åœ¨ç”»ä»€ä¹ˆå—ï¼Ÿ"
                            print("âš ï¸ æ£€æµ‹åˆ°é”™è¯¯å›å¤ï¼Œä½¿ç”¨é»˜è®¤å›å¤")
                        } else {
                            finalReplyText = replyText
                        }
                        
                        // æ·»åŠ AIå›å¤æ¶ˆæ¯
                        let aiMessage = EnhancedChatMessage(
                            text: finalReplyText,
                            isUser: false,
                            messageType: .imageAnalysis,
                            imageData: image
                        )
                        
                        self.messages.append(aiMessage)
                        
                        // ä¸ºç”»å¸ƒåˆ†æå›å¤ç”ŸæˆTTSè¯­éŸ³ï¼ˆä»…åœ¨TTSå¯ç”¨æ—¶ï¼‰
                        if !finalReplyText.isEmpty && self.navigationManager.isTTSEnabled {
                            print("ğŸµ ä¸ºç”»å¸ƒåˆ†æå›å¤ç”ŸæˆTTSè¯­éŸ³ï¼Œæ–‡æœ¬: \(finalReplyText.prefix(50))...")
                            self.apiClient.generateTTS(text: finalReplyText) { ttsResult in
                                DispatchQueue.main.async {
                                    switch ttsResult {
                                    case .success(let audioData):
                                        print("ğŸµ ç”»å¸ƒåˆ†æTTSæˆåŠŸï¼Œæ’­æ”¾éŸ³é¢‘ï¼Œå¤§å°: \(audioData.count) bytes")
                                        self.playAudioFromData(audioData)
                                    case .failure(let error):
                                        print("âŒ ç”»å¸ƒåˆ†æTTSå¤±è´¥: \(error.localizedDescription)")
                                    }
                                }
                            }
                        } else if !finalReplyText.isEmpty && !self.navigationManager.isTTSEnabled {
                            print("ğŸ”‡ TTSå·²ç¦ç”¨ï¼Œè·³è¿‡ç”»å¸ƒåˆ†æè¯­éŸ³ç”Ÿæˆ")
                        }
                        
                        // æ›´æ–°æœ€ååˆ†ææ—¶é—´
                        self.lastAnalysisTime = Date()
                        
                        print("âœ… ç”»å¸ƒåˆ†æå®Œæˆï¼Œè§†è§‰æè¿°: \(response.visionDesc)")
                        print("âœ… AIå›å¤å·²æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼Œå†…å®¹: \(finalReplyText)")
                        print("âœ… å½“å‰æ¶ˆæ¯æ€»æ•°: \(self.messages.count)")
                        print("âœ… æœ€æ–°æ¶ˆæ¯ID: \(aiMessage.id)")
                        print("âœ… æœ€æ–°æ¶ˆæ¯ç±»å‹: \(aiMessage.messageType)")
                        print("âœ… æœ€æ–°æ¶ˆæ¯æ—¶é—´: \(aiMessage.timestamp)")
                        
                    } else {
                        let errorMessage = EnhancedChatMessage(
                            text: "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨çœ‹ä¸æ¸…æ¥šï¼Œèƒ½å†è¯•ä¸€æ¬¡å—ï¼Ÿ",
                            isUser: false,
                            messageType: .error
                        )
                        messages.append(errorMessage)
                    }
                case .failure(let error):
                    print("âŒ ç”»å¸ƒåˆ†æå¤±è´¥: \(error.localizedDescription)")
                    let errorMessage = EnhancedChatMessage(
                        text: "ç½‘ç»œè¿æ¥æœ‰é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ï½",
                        isUser: false,
                        messageType: .error
                    )
                    messages.append(errorMessage)
                }
            }
        }
    }
    
    private func createCanvasScreenshot() -> UIImage? {
        let canvasContent = ZStack {
            Rectangle()
                .fill(Color.white)
                .cornerRadius(12)
            
            ForEach(paths) { segment in
                Path { p in
                    if let first = segment.points.first {
                        p.move(to: first)
                        for point in segment.points.dropFirst() {
                            p.addLine(to: point)
                        }
                    }
                }
                .stroke(segment.color, lineWidth: segment.lineWidth)
            }
        }
        .frame(width: 800, height: 600)
        .background(Color.white)
        
        let renderer = ImageRenderer(content: canvasContent)
        return renderer.uiImage
    }
    
    // MARK: - è‡ªåŠ¨åˆ†æåŠŸèƒ½
    
    private func startAutoAnalysis() {
        stopAutoAnalysis()
        autoAnalysisTimer = Timer.scheduledTimer(withTimeInterval: 5.0, repeats: true) { _ in
            checkForAutoAnalysis()
        }
    }
    
    private func stopAutoAnalysis() {
        autoAnalysisTimer?.invalidate()
        autoAnalysisTimer = nil
    }
    
    private func handleCanvasChange() {
        // å½“ç”»å¸ƒæœ‰æ–°å†…å®¹æ—¶ï¼Œé‡ç½®åˆ†æè®¡æ—¶
        if !paths.isEmpty {
            lastAnalysisTime = nil
        }
    }
    
    private func checkForAutoAnalysis() {
        guard !paths.isEmpty, !isLoading else { return }
        
        let now = Date()
        let shouldAnalyze: Bool
        
        if let lastTime = lastAnalysisTime {
            // å¦‚æœè¶…è¿‡åˆ†æé—´éš”ï¼Œè¿›è¡Œæ–°åˆ†æ
            shouldAnalyze = now.timeIntervalSince(lastTime) > analysisInterval
        } else {
            // å¦‚æœä»æœªåˆ†æè¿‡ï¼Œæ£€æŸ¥ç”»å¸ƒæ˜¯å¦æœ‰è¶³å¤Ÿå†…å®¹
            shouldAnalyze = paths.count >= 3 // è‡³å°‘æœ‰3æ¡è·¯å¾„
        }
        
        if shouldAnalyze {
            // å‘é€æ™ºèƒ½æé—®
            sendSmartQuestion()
        }
    }
    
    private func sendSmartQuestion() {
        let questions = [
            "æˆ‘çœ‹åˆ°ä½ ç”»äº†ä¸€äº›å¾ˆæœ‰è¶£çš„ä¸œè¥¿ï¼èƒ½å‘Šè¯‰æˆ‘ä½ åœ¨ç”»ä»€ä¹ˆå—ï¼Ÿ",
            "ä½ çš„ç”»çœ‹èµ·æ¥å¾ˆæ£’å‘¢ï¼æƒ³åŠ ä¸€äº›é¢œè‰²æˆ–è€…ç»†èŠ‚å—ï¼Ÿ",
            "è¿™å¹…ç”»è®©æˆ‘æƒ³åˆ°äº†å¾ˆå¤šæ•…äº‹ï¼Œä½ æƒ³å¬æˆ‘åˆ†æä¸€ä¸‹å—ï¼Ÿ",
            "ç”»å¾—çœŸä¸é”™ï¼ä½ è§‰å¾—è¿˜å¯ä»¥æ·»åŠ ä»€ä¹ˆæ¥è®©å®ƒæ›´ä¸°å¯Œå‘¢ï¼Ÿ",
            "ä½ çš„åˆ›æ„å¾ˆæ£’ï¼è¦ä¸è¦æˆ‘ç»™ä½ ä¸€äº›ç»˜ç”»å»ºè®®ï¼Ÿ"
        ]
        
        let randomQuestion = questions.randomElement() ?? "ç”»å¾—çœŸä¸é”™ï¼è¦ä¸è¦å’Œæˆ‘èŠèŠä½ çš„ä½œå“ï¼Ÿ"
        
        let questionMessage = EnhancedChatMessage(
            text: randomQuestion,
            isUser: false,
            messageType: .smartQuestion
        )
        
        DispatchQueue.main.async {
            self.messages.append(questionMessage)
            self.lastAnalysisTime = Date()
        }
    }
    
    // MARK: - å½•éŸ³ä¸è¯­éŸ³è¯†åˆ«
    private func startRecording() {
        print("ğŸ¤ å¼€å§‹å½•éŸ³æµç¨‹...")
        recognitionError = nil
        
        // å…ˆæ£€æŸ¥å½“å‰æƒé™çŠ¶æ€
        let currentStatus = AVAudioSession.sharedInstance().recordPermission
        print("ğŸ¤ å½“å‰éº¦å…‹é£æƒé™çŠ¶æ€: \(currentStatus.rawValue)")
        
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            DispatchQueue.main.async {
                print("ğŸ¤ æƒé™è¯·æ±‚ç»“æœ: \(granted)")
                if granted {
                    print("ğŸ¤ æƒé™å·²è·å¾—ï¼Œå¼€å§‹å½•éŸ³...")
                    self.audioRecorder.startRecording()
                } else {
                    print("ğŸ¤ æƒé™è¢«æ‹’ç»")
                    self.recognitionError = "è¯·åœ¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®"
                }
            }
        }
    }
    private func stopRecordingAndRecognize() {
        audioRecorder.stopRecording()
        guard let url = audioRecorder.audioURL else {
            recognitionError = "å½•éŸ³å¤±è´¥ï¼Œè¯·é‡è¯•"
            return
        }
        isUploading = true
        uploadAudioForRecognition(audioURL: url) { result in
            DispatchQueue.main.async {
                self.isUploading = false
                switch result {
                case .success(let text):
                    if text.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
                        self.recognitionError = "æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³"
                    } else {
                        // ç›´æ¥å‘é€ç»™LLMï¼Œä¸å†å¡«å…¥è¾“å…¥æ¡†
                        let userMessage = EnhancedChatMessage(text: text, isUser: true, messageType: .text)
                        self.messages.append(userMessage)
                        self.sendRecognizedText(text)
                    }
                case .failure(_):
                    self.recognitionError = "è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•"
                }
                // æ¸…ç†å½•éŸ³
                self.audioRecorder.deleteRecording()
            }
        }
    }
    
    private func sendRecognizedText(_ text: String) {
        isLoading = true
        apiClient.sendTextMessage(text, observeCanvasHandler: { sessionId in
            observeCanvasAndReply(sessionId: sessionId)
        }) { result, rawJson in
            DispatchQueue.main.async {
                self.isLoading = false
                // æ£€æŸ¥rawJsonä¸­çš„audio_data
                if let audioBase64 = rawJson?["audio_data"] as? String, !audioBase64.isEmpty {
                    print("æ”¶åˆ°éŸ³é¢‘æ•°æ®ï¼Œé•¿åº¦: \(audioBase64.count) å­—ç¬¦")
                    // æ’­æ”¾ä»åç«¯è¿”å›çš„éŸ³é¢‘
                    self.playAudioFromBase64(audioBase64)
                    print("âœ… å·²æ’­æ”¾åç«¯è¿”å›çš„éŸ³é¢‘")
                } else {
                    print("æœªæ£€æµ‹åˆ°audio_dataæˆ–å†…å®¹ä¸ºç©º")
                }
                switch result {
                case .success(let response):
                    if response.success {
                        let aiMessage = EnhancedChatMessage(
                            text: response.response,
                            isUser: false,
                            messageType: .text
                        )
                        
                        // ä¸ºAIå›å¤ç”ŸæˆTTSè¯­éŸ³ï¼ˆä»…åœ¨TTSå¯ç”¨æ—¶ï¼‰
                        if !response.response.isEmpty && self.navigationManager.isTTSEnabled {
                            print("ğŸµ ä¸ºAIå›å¤ç”ŸæˆTTSè¯­éŸ³ï¼Œæ–‡æœ¬: \(response.response.prefix(50))...")
                            self.apiClient.generateTTS(text: response.response) { ttsResult in
                                DispatchQueue.main.async {
                                    switch ttsResult {
                                    case .success(let audioData):
                                        print("ğŸµ AIå›å¤TTSæˆåŠŸï¼Œæ’­æ”¾éŸ³é¢‘ï¼Œå¤§å°: \(audioData.count) bytes")
                                        self.playAudioFromData(audioData)
                                    case .failure(let error):
                                        print("âŒ AIå›å¤TTSå¤±è´¥: \(error.localizedDescription)")
                                    }
                                }
                            }
                        } else if !response.response.isEmpty && !self.navigationManager.isTTSEnabled {
                            print("ğŸ”‡ TTSå·²ç¦ç”¨ï¼Œè·³è¿‡AIå›å¤è¯­éŸ³ç”Ÿæˆ")
                        }
                        
                        self.messages.append(aiMessage)
                    } else {
                        let errorMessage = EnhancedChatMessage(
                            text: "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å›°æƒ‘ã€‚èƒ½å†è¯•ä¸€æ¬¡å—ï¼Ÿ",
                            isUser: false,
                            messageType: .error
                        )
                        self.messages.append(errorMessage)
                    }
                case .failure(_):
                    let errorMessage = EnhancedChatMessage(
                        text: "ç½‘ç»œè¿æ¥æœ‰é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ï½",
                        isUser: false,
                        messageType: .error
                    )
                    self.messages.append(errorMessage)
                }
            }
        }
    }
    
    // MARK: - ä¸Šä¼ éŸ³é¢‘åˆ°åç«¯è¯†åˆ«
    private func uploadAudioForRecognition(audioURL: URL, completion: @escaping (Result<String, Error>) -> Void) {
        let url = URL(string: "http://10.4.176.7:8000/api/speech-to-text")!
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        let boundary = "Boundary-\(UUID().uuidString)"
        request.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")
        var body = Data()
        let filename = audioURL.lastPathComponent
        let mimetype = "audio/wav"
        let fileData = try? Data(contentsOf: audioURL)
        body.append("--\(boundary)\r\n".data(using: .utf8)!)
        body.append("Content-Disposition: form-data; name=\"file\"; filename=\"\(filename)\"\r\n".data(using: .utf8)!)
        body.append("Content-Type: \(mimetype)\r\n\r\n".data(using: .utf8)!)
        body.append(fileData ?? Data())
        body.append("\r\n".data(using: .utf8)!)
        body.append("--\(boundary)--\r\n".data(using: .utf8)!)
        request.httpBody = body
        URLSession.shared.dataTask(with: request) { data, response, error in
            if let error = error {
                completion(.failure(error))
                return
            }
            guard let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let success = json["success"] as? Bool, success,
                  let text = json["text"] as? String else {
                completion(.failure(NSError(domain: "SpeechToText", code: -1, userInfo: nil)))
                return
            }
            completion(.success(text))
        }.resume()
    }
    
    private func observeCanvasAndReply(sessionId: String?) {
        self.isObservingCanvas = true
        // æˆªå›¾
        let canvasImage = createCanvasScreenshot()
        guard let image = canvasImage else {
            self.isObservingCanvas = false
            return
        }
        // å›ä¼ ç»™åç«¯ï¼Œå¸¦session_id
        apiClient.analyzeImage(image) { result, rawJson in
            DispatchQueue.main.async {
                self.isObservingCanvas = false
                // å›ä¼ åå¯è‡ªåŠ¨è§¦å‘åˆ†æ/å¯¹è¯æµç¨‹ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
                // è¿™é‡Œå¯æ ¹æ®å®é™…éœ€æ±‚ç»§ç»­åç»­æµç¨‹
            }
        }
    }
    
    private func playAudioFromBase64(_ base64String: String) {
        guard let audioData = Data(base64Encoded: base64String) else {
            print("Base64è§£ç å¤±è´¥")
            return
        }
        playAudioFromData(audioData)
    }
    
    private func playAudioFromData(_ audioData: Data) {
        do {
            // ç¡®ä¿éŸ³é¢‘ä¼šæ’­æ”¾
            try? AVAudioSession.sharedInstance().setCategory(.playback)
            let player = try AVAudioPlayer(data: audioData)
            player.prepareToPlay()
            player.play()
            self.audioPlayer = player // å¿…é¡»æŒæœ‰å¼•ç”¨
            print("éŸ³é¢‘å·²å¼€å§‹æ’­æ”¾ï¼Œé•¿åº¦: \(audioData.count) å­—èŠ‚")
        } catch {
            print("éŸ³é¢‘æ’­æ”¾å¤±è´¥: \(error)")
        }
    }
}

// MARK: - å¢å¼ºæ¶ˆæ¯æ¨¡å‹

struct EnhancedChatMessage: Identifiable {
    let id = UUID()
    let text: String
    let isUser: Bool
    let timestamp = Date()
    let messageType: MessageType
    let imageData: UIImage?
    
    init(text: String, isUser: Bool, messageType: MessageType, imageData: UIImage? = nil) {
        self.text = text
        self.isUser = isUser
        self.messageType = messageType
        self.imageData = imageData
    }
    
    enum MessageType {
        case text
        case imageAnalysis
        case smartQuestion
        case analysis
        case error
    }
}

// MARK: - æ‚¬æµ®å¯¹è¯æ°”æ³¡ç»„ä»¶
struct FloatingChatBubble: View {
    let message: EnhancedChatMessage
    
    var body: some View {
        HStack {
            if message.isUser {
                Spacer()
                VStack(alignment: .trailing, spacing: 4) {
                    Text(message.text)
                        .font(.body)
                        .foregroundColor(.white)
                        .padding(.horizontal, 12)
                        .padding(.vertical, 8)
                        .background(
                            RoundedRectangle(cornerRadius: 16)
                                .fill(Color.blue)
                        )
                    
                    Text(formatTime(message.timestamp))
                        .font(.caption2)
                        .foregroundColor(.secondary)
                }
            } else {
                VStack(alignment: .leading, spacing: 4) {
                    HStack(spacing: 6) {
                        // æ¶ˆæ¯ç±»å‹å›¾æ ‡
                        Group {
                            switch message.messageType {
                            case .imageAnalysis:
                                Image(systemName: "eye.fill")
                                    .foregroundColor(.blue)
                            case .smartQuestion:
                                Image(systemName: "questionmark.circle.fill")
                                    .foregroundColor(.orange)
                            case .analysis:
                                Image(systemName: "magnifyingglass")
                                    .foregroundColor(.purple)
                            case .error:
                                Image(systemName: "exclamationmark.triangle.fill")
                                    .foregroundColor(.red)
                            default:
                                Image(systemName: "message.fill")
                                    .foregroundColor(.green)
                            }
                        }
                        .font(.caption)
                        
                        Text(message.text)
                            .font(.body)
                            .foregroundColor(.primary)
                            .padding(.horizontal, 12)
                            .padding(.vertical, 8)
                            .background(
                                RoundedRectangle(cornerRadius: 16)
                                    .fill(backgroundColorForType(message.messageType))
                            )
                    }
                    
                    Text(formatTime(message.timestamp))
                        .font(.caption2)
                        .foregroundColor(.secondary)
                        .padding(.leading, 20)
                }
                Spacer()
            }
        }
        .padding(.horizontal, 4)
    }
    
    private func backgroundColorForType(_ type: EnhancedChatMessage.MessageType) -> Color {
        switch type {
        case .imageAnalysis:
            return Color.blue.opacity(0.1)
        case .smartQuestion:
            return Color.orange.opacity(0.1)
        case .analysis:
            return Color.purple.opacity(0.1)
        case .error:
            return Color.red.opacity(0.1)
        default:
            return Color(.systemGray6)
        }
    }
    
    private func formatTime(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.timeStyle = .short
        return formatter.string(from: date)
    }
}

#Preview {
    ChatbotView(canvasImage: .constant(nil), paths: .constant([]), isObservingCanvas: .constant(false))
        .padding()
        .background(Color.gray.opacity(0.1))
} 